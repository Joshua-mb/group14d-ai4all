{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Apple Stock Prediction - Neural Network Classifier\n",
    "## Predicting tomorrow's stock direction using sentiment + technical indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data_header",
   "metadata": {},
   "source": [
    "## 1. Load and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "load_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (252, 16)\n",
      "\n",
      "Columns: ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Stock Name_x', 'price_change', 'target', 'sentiment_negative', 'sentiment_neutral', 'sentiment_positive', 'sentiment_compound', 'Stock Name_y', 'sentiment_label']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Stock Name_x</th>\n",
       "      <th>price_change</th>\n",
       "      <th>target</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_compound</th>\n",
       "      <th>Stock Name_y</th>\n",
       "      <th>sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>143.660004</td>\n",
       "      <td>144.380005</td>\n",
       "      <td>141.279999</td>\n",
       "      <td>141.500000</td>\n",
       "      <td>140.478485</td>\n",
       "      <td>89056700</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.051286</td>\n",
       "      <td>0.851143</td>\n",
       "      <td>0.097571</td>\n",
       "      <td>0.098900</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-01</td>\n",
       "      <td>141.899994</td>\n",
       "      <td>142.919998</td>\n",
       "      <td>139.110001</td>\n",
       "      <td>142.649994</td>\n",
       "      <td>141.620163</td>\n",
       "      <td>94639600</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1.149994</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.024455</td>\n",
       "      <td>0.872455</td>\n",
       "      <td>0.103182</td>\n",
       "      <td>0.248255</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>141.759995</td>\n",
       "      <td>142.210007</td>\n",
       "      <td>138.270004</td>\n",
       "      <td>139.139999</td>\n",
       "      <td>138.135513</td>\n",
       "      <td>98322000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>-3.509995</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>0.122830</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-10-05</td>\n",
       "      <td>139.490005</td>\n",
       "      <td>142.240005</td>\n",
       "      <td>139.360001</td>\n",
       "      <td>141.110001</td>\n",
       "      <td>140.091278</td>\n",
       "      <td>80861100</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1.970001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>0.331000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-10-06</td>\n",
       "      <td>139.470001</td>\n",
       "      <td>142.149994</td>\n",
       "      <td>138.369995</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>140.974869</td>\n",
       "      <td>83221100</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.889999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.910800</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0.243520</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Open        High         Low       Close   Adj Close  \\\n",
       "0  2021-09-30  143.660004  144.380005  141.279999  141.500000  140.478485   \n",
       "1  2021-10-01  141.899994  142.919998  139.110001  142.649994  141.620163   \n",
       "2  2021-10-04  141.759995  142.210007  138.270004  139.139999  138.135513   \n",
       "3  2021-10-05  139.490005  142.240005  139.360001  141.110001  140.091278   \n",
       "4  2021-10-06  139.470001  142.149994  138.369995  142.000000  140.974869   \n",
       "\n",
       "     Volume Stock Name_x  price_change  target  sentiment_negative  \\\n",
       "0  89056700         AAPL           NaN     NaN            0.051286   \n",
       "1  94639600         AAPL      1.149994     1.0            0.024455   \n",
       "2  98322000         AAPL     -3.509995    -1.0            0.030900   \n",
       "3  80861100         AAPL      1.970001     1.0            0.042200   \n",
       "4  83221100         AAPL      0.889999     1.0            0.012200   \n",
       "\n",
       "   sentiment_neutral  sentiment_positive  sentiment_compound Stock Name_y  \\\n",
       "0           0.851143            0.097571            0.098900         AAPL   \n",
       "1           0.872455            0.103182            0.248255         AAPL   \n",
       "2           0.910000            0.059300            0.122830         AAPL   \n",
       "3           0.890000            0.067800            0.331000         AAPL   \n",
       "4           0.910800            0.077000            0.243520         AAPL   \n",
       "\n",
       "  sentiment_label  \n",
       "0        Positive  \n",
       "1        Positive  \n",
       "2        Positive  \n",
       "3        Positive  \n",
       "4        Positive  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your data files\n",
    "aapl_finance_df = pd.read_csv('../AAPL Data/AAPL_finance_data.csv')\n",
    "aapl_sentiment_df = pd.read_csv('../AAPL Data/AAPL_avg_sentiment_data.csv')\n",
    "\n",
    "# Merge on date\n",
    "merged_aapl_df = pd.merge(aapl_finance_df, aapl_sentiment_df, on='Date', how='inner')\n",
    "\n",
    "print(f\"Dataset shape: {merged_aapl_df.shape}\")\n",
    "print(f\"\\nColumns: {list(merged_aapl_df.columns)}\")\n",
    "merged_aapl_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_eng_header",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feature_engineering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After feature engineering: (232, 32)\n",
      "\n",
      "Target distribution:\n",
      "Target\n",
      "0    117\n",
      "1    115\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage of UP days: 49.57%\n"
     ]
    }
   ],
   "source": [
    "# Create a copy for processing\n",
    "df = merged_aapl_df.copy()\n",
    "\n",
    "# Convert Date to datetime and sort chronologically\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "# Technical indicators (if not already in your finance data)\n",
    "# Price-based features\n",
    "if 'Price_Range' not in df.columns:\n",
    "    df['Price_Range'] = df['High'] - df['Low']\n",
    "if 'Price_Change' not in df.columns:\n",
    "    df['Price_Change'] = df['Close'] - df['Open']\n",
    "\n",
    "# Volume indicators\n",
    "if 'Volume_MA_5' not in df.columns:\n",
    "    df['Volume_MA_5'] = df['Volume'].rolling(window=5).mean()\n",
    "if 'Volume_MA_10' not in df.columns:\n",
    "    df['Volume_MA_10'] = df['Volume'].rolling(window=10).mean()\n",
    "\n",
    "# Moving averages\n",
    "if 'MA_5' not in df.columns:\n",
    "    df['MA_5'] = df['Close'].rolling(window=5).mean()\n",
    "if 'MA_10' not in df.columns:\n",
    "    df['MA_10'] = df['Close'].rolling(window=10).mean()\n",
    "if 'MA_20' not in df.columns:\n",
    "    df['MA_20'] = df['Close'].rolling(window=20).mean()\n",
    "\n",
    "# Momentum\n",
    "if 'Momentum_5' not in df.columns:\n",
    "    df['Momentum_5'] = df['Close'].pct_change(periods=5)\n",
    "if 'Momentum_10' not in df.columns:\n",
    "    df['Momentum_10'] = df['Close'].pct_change(periods=10)\n",
    "\n",
    "# Volatility\n",
    "if 'Volatility_5' not in df.columns:\n",
    "    df['Volatility_5'] = df['Close'].rolling(window=5).std()\n",
    "if 'Volatility_10' not in df.columns:\n",
    "    df['Volatility_10'] = df['Close'].rolling(window=10).std()\n",
    "\n",
    "# RSI\n",
    "if 'RSI' not in df.columns:\n",
    "    delta = df['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# MACD\n",
    "if 'MACD' not in df.columns:\n",
    "    exp1 = df['Close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['Close'].ewm(span=26, adjust=False).mean()\n",
    "    df['MACD'] = exp1 - exp2\n",
    "    df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# Target: Will price go UP tomorrow? (CRITICAL: predict TOMORROW using TODAY's data)\n",
    "df['Tomorrow_Close'] = df['Close'].shift(-1)  # Tomorrow's closing price\n",
    "df['Target'] = (df['Tomorrow_Close'] > df['Close']).astype(int)  # 1 if up, 0 if down\n",
    "\n",
    "# Drop rows with NaN\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"\\nAfter feature engineering: {df.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['Target'].value_counts())\n",
    "print(f\"\\nPercentage of UP days: {df['Target'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepare_header",
   "metadata": {},
   "source": [
    "## 3. Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "prepare_features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found sentiment columns: ['sentiment_negative', 'sentiment_neutral', 'sentiment_positive', 'sentiment_compound', 'sentiment_label']\n",
      "\n",
      "Using 23 features:\n",
      "['Open', 'High', 'Low', 'Volume', 'Price_Range', 'Price_Change', 'Volume_MA_5', 'Volume_MA_10', 'MA_5', 'MA_10', 'MA_20', 'Momentum_5', 'Momentum_10', 'Volatility_5', 'Volatility_10', 'RSI', 'MACD', 'Signal_Line', 'sentiment_negative', 'sentiment_neutral', 'sentiment_positive', 'sentiment_compound', 'sentiment_label']\n",
      "\n",
      "X shape: (232, 23)\n",
      "y shape: (232,)\n"
     ]
    }
   ],
   "source": [
    "# Define feature columns\n",
    "# Adjust these based on what columns are actually in your sentiment data\n",
    "# Common sentiment columns: 'sentiment_mean', 'sentiment_std', 'sentiment_max', 'sentiment_min'\n",
    "\n",
    "price_features = ['Open', 'High', 'Low', 'Volume', 'Price_Range', 'Price_Change',\n",
    "                  'Volume_MA_5', 'Volume_MA_10', 'MA_5', 'MA_10', 'MA_20',\n",
    "                  'Momentum_5', 'Momentum_10', 'Volatility_5', 'Volatility_10',\n",
    "                  'RSI', 'MACD', 'Signal_Line']\n",
    "\n",
    "# Find sentiment columns (adjust column names if needed)\n",
    "sentiment_cols = [col for col in df.columns if 'sentiment' in col.lower()]\n",
    "print(f\"Found sentiment columns: {sentiment_cols}\")\n",
    "\n",
    "# Combine all features\n",
    "feature_columns = price_features + sentiment_cols\n",
    "\n",
    "# Check which features actually exist in your data\n",
    "feature_columns = [col for col in feature_columns if col in df.columns]\n",
    "print(f\"\\nUsing {len(feature_columns)} features:\")\n",
    "print(feature_columns)\n",
    "\n",
    "X = df[feature_columns].values\n",
    "y = df['Target'].values\n",
    "\n",
    "print(f\"\\nX shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split_header",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split (Chronological!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "train_test_split",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 185 samples\n",
      "Test set: 47 samples\n",
      "\n",
      "Train period: 2021-10-27 00:00:00 to 2022-07-22 00:00:00\n",
      "Test period: 2022-07-25 00:00:00 to 2022-09-28 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# CRITICAL: Chronological split for time series (80/20)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"\\nTrain period: {df['Date'].iloc[0]} to {df['Date'].iloc[split_idx-1]}\")\n",
    "print(f\"Test period: {df['Date'].iloc[split_idx]} to {df['Date'].iloc[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scale_header",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling (CRITICAL for Neural Networks!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "feature_scaling",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Neutral'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Neural networks REQUIRE scaled features!\u001b[39;00m\n\u001b[1;32m      2\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m----> 3\u001b[0m X_train_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScaled training data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train_scaled\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofSt.Thomas(2)/AI4all Project 3/group14d-ai4all/venv/lib/python3.9/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofSt.Thomas(2)/AI4all Project 3/group14d-ai4all/venv/lib/python3.9/site-packages/sklearn/base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    904\u001b[0m             (\n\u001b[1;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    914\u001b[0m         )\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofSt.Thomas(2)/AI4all Project 3/group14d-ai4all/venv/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:894\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofSt.Thomas(2)/AI4all Project 3/group14d-ai4all/venv/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofSt.Thomas(2)/AI4all Project 3/group14d-ai4all/venv/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:930\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \n\u001b[1;32m    900\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    929\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 930\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofSt.Thomas(2)/AI4all Project 3/group14d-ai4all/venv/lib/python3.9/site-packages/sklearn/utils/validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofSt.Thomas(2)/AI4all Project 3/group14d-ai4all/venv/lib/python3.9/site-packages/sklearn/utils/validation.py:1055\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1055\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m   1059\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofSt.Thomas(2)/AI4all Project 3/group14d-ai4all/venv/lib/python3.9/site-packages/sklearn/utils/_array_api.py:839\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    837\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 839\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Neutral'"
     ]
    }
   ],
   "source": [
    "# Neural networks REQUIRE scaled features!\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Scaled training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Mean of scaled features (should be ~0): {X_train_scaled.mean():.6f}\")\n",
    "print(f\"Std of scaled features (should be ~1): {X_train_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "build_model_header",
   "metadata": {},
   "source": [
    "## 6. Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "    \n",
    "    # First hidden layer\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    layers.Dropout(0.3),  # Prevent overfitting\n",
    "    \n",
    "    # Second hidden layer\n",
    "    layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    # Third hidden layer\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    \n",
    "    # Output layer (binary classification)\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_header",
   "metadata": {},
   "source": [
    "## 7. Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,  # Stop if no improvement for 15 epochs\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Neural Network...\\n\")\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,  # Use 20% of training data for validation\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluate_header",
   "metadata": {},
   "source": [
    "## 8. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"NEURAL NETWORK RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict(X_test_scaled, verbose=0)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Down (0)', 'Up (1)']))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "print(f\"\\nTrue Negatives (correctly predicted DOWN): {cm[0,0]}\")\n",
    "print(f\"False Positives (predicted UP, actually DOWN): {cm[0,1]}\")\n",
    "print(f\"False Negatives (predicted DOWN, actually UP): {cm[1,0]}\")\n",
    "print(f\"True Positives (correctly predicted UP): {cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize_header",
   "metadata": {},
   "source": [
    "## 9. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Training history - Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Training Loss', linewidth=2, color='blue')\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2, color='orange')\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 0].set_title('Model Loss During Training', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Training history - Accuracy\n",
    "axes[0, 1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2, color='green')\n",
    "axes[0, 1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2, color='red')\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0, 1].set_title('Model Accuracy During Training', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Confusion Matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0],\n",
    "            xticklabels=['Down', 'Up'], yticklabels=['Down', 'Up'])\n",
    "axes[1, 0].set_xlabel('Predicted', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Actual', fontsize=12)\n",
    "axes[1, 0].set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. Prediction probabilities\n",
    "axes[1, 1].hist(y_pred_proba, bins=30, edgecolor='black', alpha=0.7, color='purple')\n",
    "axes[1, 1].axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Threshold (0.5)')\n",
    "axes[1, 1].set_xlabel('Predicted Probability (P(Up))', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 1].set_title('Distribution of Predictions', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('AAPL_NN_Results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Visualization saved as 'AAPL_NN_Results.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare_header",
   "metadata": {},
   "source": [
    "## 10. Compare with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you ran Random Forest before, compare the results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Neural Network Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"Your Random Forest Accuracy: ~42%\")  # Update with your actual RF accuracy\n",
    "print(\"\\nKey Differences:\")\n",
    "print(\"✓ Neural Network uses feature scaling\")\n",
    "print(\"✓ Neural Network uses dropout and L2 regularization\")\n",
    "print(\"✓ Neural Network can capture non-linear patterns\")\n",
    "print(\"✓ Both models properly avoid data leakage!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_header",
   "metadata": {},
   "source": [
    "## 11. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save('AAPL_NN_Model.keras')\n",
    "print(\"✓ Model saved as 'AAPL_NN_Model.keras'\")\n",
    "\n",
    "# Save training history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv('AAPL_NN_Training_History.csv', index=False)\n",
    "print(\"✓ Training history saved as 'AAPL_NN_Training_History.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps_header",
   "metadata": {},
   "source": [
    "## Next Steps to Improve\n",
    "\n",
    "1. **Try different architectures**: More/fewer layers, different neuron counts\n",
    "2. **Hyperparameter tuning**: Learning rate, batch size, dropout rate\n",
    "3. **LSTM networks**: Better for time series data\n",
    "4. **Ensemble**: Combine Random Forest + Neural Network predictions\n",
    "5. **More features**: Add market-wide indicators, more technical indicators\n",
    "6. **Class balancing**: Handle UP/DOWN imbalance if present"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
